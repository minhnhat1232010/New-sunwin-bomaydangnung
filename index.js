// server.js
const express = require('express');
const axios = require('axios');

const app = express();
const PORT = process.env.PORT || 3000;

// --- fetch data from source API ---
async function fetchData() {
    try {
        const response = await axios.get('https://ahihidonguoccut-2b5i.onrender.com/mohobomaycai');
        return response.data;
    } catch (error) {
        console.error("Error fetching data:", error.message);
        return null;
    }
}

// --- Main predict function (entry) ---
function predict(data) {
    // Expect data.Lich_su_phien (array of {Phien, Ket_qua}) OR fallback use other fields if single session
    const historyRecords = Array.isArray(data.Lich_su_phien) ? data.Lich_su_phien : (data.history || []);
    const history = historyRecords.map(h => (h.Ket_qua || h.ket_qua || h.result || '').trim());

    if (!history || history.length < 20) {
        return {
            predict_goc: "Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu",
            tin_cay: "0%",
            tong_quat_so_do_du_doan: "Kh√¥ng ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ (c·∫ßn √≠t nh·∫•t 20 phi√™n).",
            giai_thich: "H·ªá th·ªëng y√™u c·∫ßu √≠t nh·∫•t 20 phi√™n l·ªãch s·ª≠ ƒë·ªÉ ph√¢n t√≠ch pattern v√† ch·∫°y ensemble AI."
        };
    }

    // Deterministic: extract last 20 for short-term and last 50 (or all) for long-term
    const last20 = history.slice(-20);
    const last50 = history.slice(-50);

    // Generate 20 deterministic pattern samples from history
    const patternSamples = generatePatternSamplesDeterministic(history, 20);

    // Define models (each returns { prediction, explanation })
    const models = [
        { name: 'Model 1: Pattern Analysis', func: model1PatternAnalysis },
        { name: 'Model 2: Rolling Frequency', func: model2FrequencyAnalysis },
        { name: 'Model 3: Markov Chain', func: model3MarkovChain },
        { name: 'Model 4: N-gram Matching', func: model4NgramMatching },
        { name: 'Model 5: Heuristic Ensemble', func: model5Heuristic }
    ];

    const allPredictions = [];
    const explanations = [];

    // Run ensemble: each model on short-term and long-term (deterministic)
    models.forEach((model, idx) => {
        const shortPred = model.func(last20.slice(), patternSamples, `${idx+1}-S`);
        const longPred = model.func(last50.length >= 20 ? last50.slice() : history.slice(), patternSamples, `${idx+1}-L`);
        allPredictions.push(shortPred.prediction);
        allPredictions.push(longPred.prediction);
        explanations.push(shortPred.explanation);
        explanations.push(longPred.explanation);
    });

    // Voting
    const voteTai = allPredictions.filter(p => p === 'T√†i').length;
    const voteXiu = allPredictions.filter(p => p === 'X·ªâu').length;
    const totalVotes = allPredictions.length;
    let finalResult = 'Kh√¥ng x√°c ƒë·ªãnh';
    if (voteTai > voteXiu) finalResult = 'T√†i';
    else if (voteXiu > voteTai) finalResult = 'X·ªâu';
    const confidence = Math.round((Math.max(voteTai, voteXiu) / totalVotes) * 100);

    // Build composite explanation (AI T·ªîNG H·ª¢P)
    const aiExplanation = buildAiTongHopExplanation({
        finalResult, confidence, voteTai, voteXiu, totalVotes, patternSamples, explanations, history
    });

    return {
        predict_goc: finalResult,
        tin_cay: `${confidence}%`,
        tong_quat_so_do_du_doan: `T·ª´ ${patternSamples.length} m·∫´u c·∫ßu tr√≠ch xu·∫•t, ensemble ${totalVotes} AI (2 phi√™n b·∫£n m·ªói model). K·∫øt qu·∫£: ${voteTai}/${totalVotes} vote T√†i, ${voteXiu}/${totalVotes} vote X·ªâu. D·ª± ƒëo√°n ch√≠nh: ${finalResult}.`,
        giai_thich: aiExplanation
    };
}

// ----------------- Deterministic Pattern Sample Generator -----------------
function generatePatternSamplesDeterministic(fullHistory, sampleCount = 20) {
    // Build counts of sliding windows of length 3,4,5 (prefer longer)
    const windows = {};
    for (let n of [5,4,3]) {
        for (let i = 0; i <= fullHistory.length - n - 1; i++) {
            const key = fullHistory.slice(i, i + n).join('-');
            const next = fullHistory[i + n];
            if (!windows[key]) windows[key] = { total: 0, nextCounts: { 'T√†i':0, 'X·ªâu':0 } };
            windows[key].total++;
            if (next === 'T√†i' || next === 'X·ªâu') windows[key].nextCounts[next]++;
        }
    }

    // Convert windows to array, sort by (total * entropy reduction) -> prefer frequent and decisive patterns
    const windowList = Object.entries(windows).map(([pattern, info]) => {
        const taiNext = info.nextCounts['T√†i'];
        const xiuNext = info.nextCounts['X·ªâu'];
        const decisive = Math.abs(taiNext - xiuNext);
        return { pattern, total: info.total, taiNext, xiuNext, decisive };
    }).sort((a,b) => (b.total * b.decisive) - (a.total * a.decisive));

    // Build samples deterministically: take top patterns, and also construct complementary types (b·ªát, 1-1, 2-2, complex)
    const samples = [];
    // 1) Top explicit patterns
    for (let i = 0; i < windowList.length && samples.length < Math.floor(sampleCount*0.6); i++) {
        const w = windowList[i];
        const next = w.taiNext >= w.xiuNext ? 'T√†i' : 'X·ªâu';
        samples.push({ type: 'match', pattern: w.pattern, next });
    }

    // 2) Detect runs (b·ªát)
    const runs = detectRuns(fullHistory);
    for (let r of runs.slice(0, Math.floor(sampleCount*0.15))) {
        samples.push({ type: 'b·ªát', pattern: r.pattern, next: r.predNext });
    }

    // 3) Detect alternations (1-1)
    const alternations = detectAlternations(fullHistory);
    for (let a of alternations.slice(0, Math.floor(sampleCount*0.1))) {
        samples.push({ type: '1-1', pattern: a.pattern, next: a.predNext });
    }

    // 4) Add balanced canonical patterns if we still need to fill
    const canonical = [
        { pattern: 'T√†i-X·ªâu-T√†i-X·ªâu', next: 'T√†i' },
        { pattern: 'X·ªâu-T√†i-X·ªâu-T√†i', next: 'X·ªâu' },
        { pattern: 'T√†i-T√†i-X·ªâu-X·ªâu', next: 'T√†i' },
        { pattern: 'X·ªâu-X·ªâu-T√†i-T√†i', next: 'X·ªâu' },
        { pattern: 'T√†i-T√†i-T√†i-X·ªâu', next: 'X·ªâu' },
        { pattern: 'X·ªâu-X·ªâu-X·ªâu-T√†i', next: 'T√†i' },
        { pattern: 'T√†i-X·ªâu-X·ªâu-T√†i', next: 'X·ªâu' },
        { pattern: 'X·ªâu-T√†i-T√†i-X·ªâu', next: 'T√†i' }
    ];
    let ci = 0;
    while (samples.length < sampleCount && ci < canonical.length) {
        samples.push(Object.assign({ type: 'canonical' }, canonical[ci]));
        ci++;
    }

    // 5) If still less (rare), pad using last20 windows deterministically
    const last20 = fullHistory.slice(-20);
    let idx = 0;
    while (samples.length < sampleCount && idx < Math.max(1, last20.length-3)) {
        const pattern = last20.slice(idx, idx+4).join('-');
        const last = last20[idx+4] || last20[last20.length - 1];
        samples.push({ type: 'pad', pattern, next: (last === 'T√†i' ? 'T√†i' : 'X·ªâu') });
        idx++;
    }

    // Trim to requested sampleCount
    const finalSamples = samples.slice(0, sampleCount);

    // Count T√†i/X·ªâu next to log
    const taiNextCount = finalSamples.filter(s => s.next === 'T√†i').length;
    console.log(`Deterministic samples generated: ${finalSamples.length} (T√†i next: ${taiNextCount}, X·ªâu next: ${finalSamples.length - taiNextCount})`);
    return finalSamples;
}

function detectRuns(history) {
    const runs = [];
    let i = 0;
    while (i < history.length) {
        let j = i+1;
        while (j < history.length && history[j] === history[i]) j++;
        const length = j - i;
        if (length >= 3) {
            const pattern = history.slice(i, j).join('-');
            const predNext = (history[j] ? history[j] : (history[i] === 'T√†i' ? 'X·ªâu' : 'T√†i')); // deterministic guess: invert next if infinite run end
            runs.push({ start: i, length, pattern, predNext });
        }
        i = j;
    }
    // sort by length desc
    return runs.sort((a,b) => b.length - a.length);
}

function detectAlternations(history) {
    // find places with strict alternation of length >=4 (T-X-T-X)
    const alts = [];
    for (let i = 0; i <= history.length - 4; i++) {
        const seq = history.slice(i, i+4);
        const isAlt = seq[0] !== seq[1] && seq[0] === seq[2] && seq[1] === seq[3];
        if (isAlt) {
            const pattern = seq.join('-');
            const predNext = seq[3] === 'T√†i' ? 'X·ªâu' : 'T√†i';
            alts.push({ index: i, pattern, predNext });
        }
    }
    return alts;
}

// ----------------- Models -----------------

// Model 1 - Pattern Analysis (detect many bridge types)
function model1PatternAnalysis(data, samples, modelId) {
    const last6 = data.slice(-6).join('-');
    let prediction = 'T√†i'; // default safe
    let explanation = '';

    // deterministic checks from highest-specificity patterns to low
    if (last6.includes('T√†i-X·ªâu-T√†i-X·ªâu')) { // 1-1
        prediction = last6.endsWith('T√†i') ? 'X·ªâu' : 'T√†i';
        explanation = `Model ${modelId} (Pattern): Ph√°t hi·ªán 1-1 (so le) trong recent "${last6}". Theo quy lu·∫≠t ƒë·∫£o, d·ª± ƒëo√°n ${prediction}.`;
    } else if (last6.includes('T√†i-X·ªâu-X·ªâu-T√†i')) { // 1-2-1
        prediction = 'X·ªâu';
        explanation = `Model ${modelId} (Pattern): M·∫´u 1-2-1 trong "${last6}" ‚Äî d·ª± ƒëo√°n g√£y sang ${prediction}.`;
    } else if (last6.includes('X·ªâu-T√†i-T√†i-X·ªâu')) { // 2-1-2
        prediction = 'T√†i';
        explanation = `Model ${modelId} (Pattern): M·∫´u 2-1-2 => ti·∫øp theo c√≥ xu h∆∞·ªõng ${prediction}.`;
    } else if (last6.match(/T√†i-T√†i-T√†i/) || last6.match(/X·ªâu-X·ªâu-X·ªâu/)) { // b·ªát 3+
        prediction = data[data.length - 1]; // ti·∫øp t·ª•c b·ªát
        explanation = `Model ${modelId} (Pattern): B·ªát >3 ph√°t hi·ªán trong "${last6}". M√¥ h√¨nh d·ª± b√°o ti·∫øp t·ª•c b·ªát: ${prediction}, nh∆∞ng c·∫£nh b√°o g√£y n·∫øu chu·ªói c√†ng d√†i.`;
    } else if (last6.match(/T√†i-T√†i-X·ªâu-X·ªâu/)) { // 2-2
        prediction = last6.endsWith('T√†i') ? 'X·ªâu' : 'T√†i';
        explanation = `Model ${modelId} (Pattern): C·∫ßu 2-2 trong "${last6}", th√¥ng th∆∞·ªùng ƒë·∫£o sau nh√≥m: ${prediction}.`;
    } else {
        // fallback: check samples majority
        const taiCount = samples.filter(s => s.next === 'T√†i').length;
        prediction = taiCount >= samples.length/2 ? 'T√†i' : 'X·ªâu';
        explanation = `Model ${modelId} (Pattern): Kh√¥ng match c·∫ßu c·ª• th·ªÉ trong "${last6}". D·ª±a v√†o ${samples.length} m·∫´u, ${taiCount} m·∫´u lead ƒë·∫øn T√†i => d·ª± ƒëo√°n ${prediction}.`;
    }

    // Add sample support %
    const sampleTaiPct = Math.round(samples.filter(s => s.next === 'T√†i').length / samples.length * 100);
    explanation += ` H·ªó tr·ª£ t·ª´ m·∫´u: ${sampleTaiPct}% d·∫´n t·ªõi T√†i.`;

    return { prediction, explanation };
}

// Model 2 - Rolling Frequency
function model2FrequencyAnalysis(data, samples, modelId) {
    if (!data.length) return { prediction: 'T√†i', explanation: `Model ${modelId} (Frequency): Kh√¥ng c√≥ d·ªØ li·ªáu.` };
    const total = data.length;
    const taiCount = data.filter(x => x === 'T√†i').length;
    const pctTai = Math.round(taiCount / total * 100);
    let prediction;
    let explanation = `Model ${modelId} (Frequency): Trong ${total} phi√™n g·∫ßn nh·∫•t, T√†i = ${pctTai}%. `;

    // deterministic balancing logic: if heavily skewed -> predict opposite (c√¢n b·∫±ng),
    // if balanced -> predict according to slight trend
    if (pctTai >= 70) {
        prediction = 'X·ªâu';
        explanation += `T·ª∑ l·ªá T√†i >=70% ‚Üí k·ª≥ v·ªçng c√¢n b·∫±ng sang X·ªâu.`;
    } else if (pctTai <= 30) {
        prediction = 'T√†i';
        explanation += `T·ª∑ l·ªá T√†i <=30% ‚Üí k·ª≥ v·ªçng b√π v·ªÅ T√†i.`;
    } else {
        // small bias: if pctTai>50 then slightly favor continuation but we use balancing heuristic
        prediction = pctTai > 55 ? 'X·ªâu' : (pctTai < 45 ? 'T√†i' : (pctTai >= 50 ? 'T√†i' : 'X·ªâu'));
        explanation += `T·ª∑ l·ªá trung gian ‚Üí d√πng heuristic c√¢n b·∫±ng/ti·∫øp t·ª•c: d·ª± ƒëo√°n ${prediction}.`;
    }

    const sampleTaiPct = Math.round(samples.filter(s => s.next === 'T√†i').length / samples.length * 100);
    explanation += ` M·∫´u h·ªó tr·ª£: ${sampleTaiPct}% d·∫´n ƒë·∫øn T√†i.`;
    return { prediction, explanation };
}

// Model 3 - Markov Chain
function model3MarkovChain(data, samples, modelId) {
    if (data.length < 2) {
        return { prediction: 'T√†i', explanation: `Model ${modelId} (Markov): Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t√≠nh ma tr·∫≠n chuy·ªÉn tr·∫°ng th√°i.` };
    }
    let TT=0, TX=0, XT=0, XX=0;
    for (let i=0;i<data.length-1;i++){
        if (data[i] === 'T√†i') {
            if (data[i+1] === 'T√†i') TT++; else TX++;
        } else {
            if (data[i+1] === 'X·ªâu') XX++; else XT++;
        }
    }
    const pTT = TT + TX > 0 ? TT / (TT + TX) : 0.5;
    const pXX = XX + XT > 0 ? XX / (XX + XT) : 0.5;
    const last = data[data.length-1];
    let prediction = last === 'T√†i' ? (pTT >= 0.5 ? 'T√†i' : 'X·ªâu') : (pXX >= 0.5 ? 'X·ªâu' : 'T√†i');
    const explanation = `Model ${modelId} (Markov): P(T‚ÜíT)=${Math.round(pTT*100)}%, P(X‚ÜíX)=${Math.round(pXX*100)}%. K·∫øt qu·∫£ hi·ªán t·∫°i "${last}" ‚Üí d·ª± ƒëo√°n ${prediction}.`;
    return { prediction, explanation };
}

// Model 4 - N-gram Matching (4-gram primary, fallback 3-gram)
function model4NgramMatching(data, samples, modelId) {
    const n = Math.min(4, Math.max(3, data.length - 1)); // use up to 4
    const lastN = data.slice(-n).join('-');
    // Build n-gram map from data
    const ngramMap = {};
    for (let i=0;i<=data.length - n - 1;i++) {
        const key = data.slice(i, i+n).join('-');
        const next = data[i+n];
        if (!ngramMap[key]) ngramMap[key] = { T√†i:0, X·ªâu:0, total:0 };
        ngramMap[key][next] = (ngramMap[key][next] || 0) + 1;
        ngramMap[key].total++;
    }

    let prediction = 'T√†i';
    let explanation = `Model ${modelId} (N-gram): So kh·ªõp chu·ªói length ${n} "${lastN}". `;
    if (ngramMap[lastN]) {
        const entry = ngramMap[lastN];
        prediction = entry.T√†i >= entry.X·ªâu ? 'T√†i' : 'X·ªâu';
        explanation += `T√¨m th·∫•y ${entry.total} match: T√†i=${entry.T√†i}, X·ªâu=${entry.X·ªâu}. D·ª± ƒëo√°n ${prediction}.`;
    } else {
        // no exact match -> try shorter gram or fallback to sample majority
        const shorter = n > 3 ? data.slice(-3).join('-') : null;
        if (shorter && ngramMap[shorter]) {
            const entry = ngramMap[shorter];
            prediction = entry.T√†i >= entry.X·ªâu ? 'T√†i' : 'X·ªâu';
            explanation += `Kh√¥ng match exact, d√πng shorter "${shorter}": T√†i=${entry.T√†i}, X·ªâu=${entry.X·ªâu} => ${prediction}.`;
        } else {
            const sampleTai = samples.filter(s=>s.next==='T√†i').length;
            prediction = sampleTai >= samples.length/2 ? 'T√†i' : 'X·ªâu';
            explanation += `Kh√¥ng c√≥ match n-gram. D√πng m·∫´u: ${sampleTai}/${samples.length} lead T√†i => ${prediction}.`;
        }
    }
    return { prediction, explanation };
}

// Model 5 - Heuristic Weighted (pattern 40%, markov 30%, freq 20%, ngram 10%)
function model5Heuristic(data, samples, modelId) {
    // patternScore: fraction of samples that predict T√†i
    const patternFraction = samples.filter(s => s.next === 'T√†i').length / samples.length;
    const freqFraction = data.filter(x => x === 'T√†i').length / data.length;

    // Markov proxy: compute pTT and pXX average to produce a T√†i preference metric
    let TT=0, TX=0, XT=0, XX=0;
    for (let i=0;i<data.length-1;i++){
        if (data[i] === 'T√†i') {
            if (data[i+1] === 'T√†i') TT++; else TX++;
        } else {
            if (data[i+1] === 'X·ªâu') XX++; else XT++;
        }
    }
    const pTT = TT + TX > 0 ? TT / (TT + TX) : 0.5;
    const pXX = XX + XT > 0 ? XX / (XX + XT) : 0.5;
    // markovFavor: positive if markov favors T√†i overall
    const markovFavor = (pTT - (1 - pXX)) / 2 + 0.5; // normalized approx

    // ngram proxy: check last 4-gram support for T√†i
    const last4 = data.slice(-4).join('-');
    // compute a mini ngram vote
    let ngramFavor = 0.5;
    for (let i=0;i<=data.length-5;i++){
        const key = data.slice(i,i+4).join('-');
        const next = data[i+4];
        if (key === last4) {
            ngramFavor = (next === 'T√†i') ? 1 : 0;
            break;
        }
    }

    const totalScore = 0.4 * patternFraction + 0.3 * markovFavor + 0.2 * freqFraction + 0.1 * ngramFavor;
    const prediction = totalScore > 0.5 ? 'T√†i' : 'X·ªâu';
    const explanation = `Model ${modelId} (Heuristic): Weights => Pattern 40% (${Math.round(patternFraction*100)}%), Markov 30% (${Math.round(markovFavor*100)}%), Freq 20% (${Math.round(freqFraction*100)}%), Ngram 10% (${Math.round(ngramFavor*100)}%). T·ªïng score T√†i: ${Math.round(totalScore*100)}% ‚Üí D·ª± ƒëo√°n ${prediction}.`;

    return { prediction, explanation };
}

// ----------------- AI T·ªîNG H·ª¢P Explanation Builder -----------------
function buildAiTongHopExplanation({ finalResult, confidence, voteTai, voteXiu, totalVotes, patternSamples, explanations, history }) {
    // Short header
    let text = `AI T·ªîNG H·ª¢P: D·ª± ƒëo√°n ch√≠nh l√† **${finalResult}** (ƒë·ªô tin c·∫≠y ${confidence}%). T·ªïng phi·∫øu: T√†i ${voteTai}/${totalVotes}, X·ªâu ${voteXiu}/${totalVotes}.\n\n`;

    // Summary of pattern samples
    const taiLead = patternSamples.filter(s => s.next === 'T√†i').length;
    text += `Chi ti·∫øt m·∫´u (t·ªïng ${patternSamples.length} m·∫´u): ${taiLead} m·∫´u d·∫´n t·ªõi T√†i, ${patternSamples.length - taiLead} d·∫´n t·ªõi X·ªâu. Nh·ªØng m·∫´u top gi√∫p h·ªá th·ªëng nh·∫≠n di·ªán c√°c lo·∫°i c·∫ßu: b·ªát, 1-1, 2-2, 1-2-1, 2-1-2, 3-1, 1-3, 2-3, 3-2, 4-1, 1-4.\n\n`;

    // Attach model explanations (most load-bearing first: show up to 5-7)
    text += `Gi·∫£i th√≠ch chi ti·∫øt t·ª´ c√°c model (t√≥m t·∫Øt):\n`;
    explanations.forEach((e, i) => {
        text += `‚Ä¢ AI #${i+1}: ${e}\n`;
    });

    // Add concrete example reference (last history snippet)
    const lastWindow = history.slice(-10).join('-');
    text += `\nD·ªØ li·ªáu tham chi·∫øu (10 phi√™n g·∫ßn nh·∫•t): ${lastWindow}.\n`;

    // Final note about limits and recommendation
    text += `\nL∆∞u √Ω: M√¥ h√¨nh ph√¢n t√≠ch c·∫ßu d·ª±a tr√™n l·ªãch s·ª≠ ‚Äî kh√¥ng th·ªÉ ƒë·∫£m b·∫£o ch√≠nh x√°c 100%. Khuy·∫øn ngh·ªã: k·∫øt h·ª£p ph√¢n t√≠ch AI v·ªõi qu·∫£n l√Ω v·ªën (Kelly ho·∫∑c sizing) v√† ch·ªâ s·ª≠ d·ª•ng k·∫øt qu·∫£ nh∆∞ 1 ngu·ªìn tham kh·∫£o.\n`;

    return text;
}

// ----------------- Express endpoints -----------------
app.get('/predict-tai-xiu', async (req, res) => {
    const rawData = await fetchData();
    if (!rawData) {
        return res.status(500).json({ error: "Failed to fetch data from the source API." });
    }

    // Try to normalize incoming rawData to provide session, dice, total, result
    const session = rawData.Phien || rawData.phien || rawData.session || null;
    const dice = (rawData.Xuc_xac_1 !== undefined && rawData.Xuc_xac_2 !== undefined && rawData.Xuc_xac_3 !== undefined)
        ? `${rawData.Xuc_xac_1}-${rawData.Xuc_xac_2}-${rawData.Xuc_xac_3}`
        : (rawData.Xuc_xac || rawData.dice || null);
    const total = rawData.Tong || rawData.tong || rawData.total || null;
    const result = rawData.Ket_qua || rawData.ket_qua || rawData.result || null;

    const p = predict(rawData);

    const fullResponse = {
        session,
        dice,
        total,
        result,
        next_session: typeof session === 'number' ? session + 1 : null,
        predict_goc: p.predict_goc,
        tin_cay: p.tin_cay,
        tong_quat_so_do_du_doan: p.tong_quat_so_do_du_doan,
        giai_thich: p.giai_thich,
        id: "@ VƒÉn Nh·∫≠t T·ªõi Ng·ªß C√πng N√® ü´¶"
    };

    res.json(fullResponse);
});

app.get('/', (req, res) => {
    res.send('API is running. Use /predict-tai-xiu endpoint to get enhanced predictions (AI T·ªîNG H·ª¢P).');
});

app.listen(PORT, () => {
    console.log(`Server is running on port ${PORT}`);
});
